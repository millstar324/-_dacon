{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaMa+UNet 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn hdbscan transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install modelscope\n",
    "!pip install datasets==2.16.0\n",
    "!pip install oss2\n",
    "!pip install addict\n",
    "!pip install albumentations==0.4.6\n",
    "!pip install sortedcontainers\n",
    "!pip install yapf==0.40.1\n",
    "!pip install kornia -U\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install polygenerator lightning segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kwy00/song\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 기본 설정및 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-07 12:35:55.035568: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-07 12:35:55.049860: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-07 12:35:55.174773: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-07 12:35:55.175913: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 12:35:55.633500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import skimage\n",
    "import umap\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 설정\n",
    "BATCH_SIZE = 8  # RTX 3080 10GB 메모리에 맞게 조정\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정 제안\n",
    "BATCH_SIZE = 8 # RTX 3080 10GB에 최적화\n",
    "IMAGE_SIZE = 256  # 이미지 크기 명시\n",
    "NUM_WORKERS = 8 # 데이터 로딩 최적화\n",
    "PIN_MEMORY = True  # GPU 메모리 전송 최적화\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = sorted(glob('/home/kwy00/song/lama-with-refiner/extracted_files/train_gt/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import zipfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "import skimage\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from polygenerator import (\n",
    "    random_polygon,\n",
    "    random_star_shaped_polygon,\n",
    "    random_convex_polygon,\n",
    ")\n",
    "from sklearn.model_selection import KFold\n",
    "from skimage.metrics import structural_similarity as ski_ssim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_image(image, min_polygon_bbox_size=50):\n",
    "    # 입력 이미지의 크기 가져오기\n",
    "    width, height = image.size\n",
    "\n",
    "    while True:\n",
    "        # 랜덤한 바운딩 박스 좌표 생성\n",
    "        bbox_x1 = random.randint(0, width-min_polygon_bbox_size)\n",
    "        bbox_y1 = random.randint(0, height-min_polygon_bbox_size)\n",
    "        bbox_x2 = random.randint(bbox_x1, width)  # x1보다 큰 x2 좌표\n",
    "        bbox_y2 = random.randint(bbox_y1, height)  # y1보다 큰 y2 좌표\n",
    "\n",
    "        # 바운딩 박스가 최소 크기보다 작으면 다시 생성\n",
    "        if (bbox_x2-bbox_x1)<min_polygon_bbox_size or (bbox_y2-bbox_y1)<min_polygon_bbox_size:\n",
    "            continue\n",
    "\n",
    "        # 바운딩 박스 정보 저장\n",
    "        mask_bbox = [bbox_x1, bbox_y1, bbox_x2, bbox_y2]\n",
    "        mask_width = bbox_x2-bbox_x1\n",
    "        mask_height = bbox_y2-bbox_y1\n",
    "\n",
    "        # 랜덤한 다각형 생성을 위한 설정\n",
    "        num_points = random.randint(3,20)  # 3~20개의 꼭지점\n",
    "        # 다각형 생성 함수 랜덤 선택\n",
    "        polygon_func = random.choice([\n",
    "            random_polygon,\n",
    "            random_star_shaped_polygon,\n",
    "            random_convex_polygon\n",
    "        ])\n",
    "\n",
    "        # 0~1 스케일로 다각형 생성 후 실제 크기로 변환\n",
    "        polygon = polygon_func(num_points=num_points) #scaled 0~1\n",
    "        polygon = [(round(r*mask_width), round(c*mask_height)) for r,c in polygon]\n",
    "\n",
    "        # 다각형 마스크 생성\n",
    "        polygon_mask = skimage.draw.polygon2mask((mask_width, mask_height), polygon)\n",
    "\n",
    "        # 다각형 크기가 최소 크기 조건을 만족하면 루프 종료\n",
    "        if np.sum(polygon_mask)>(min_polygon_bbox_size//2)**2:\n",
    "            break\n",
    "\n",
    "    # 전체 이미지 크기의 마스크 생성\n",
    "    full_image_mask = np.zeros((width, height), dtype=np.uint8)\n",
    "    full_image_mask[bbox_x1:bbox_x2, bbox_y1:bbox_y2] = polygon_mask\n",
    "\n",
    "    # 그레이스케일 이미지 생성 및 마스크 적용\n",
    "    image_gray = image.convert('L')  # RGB를 그레이스케일로 변환\n",
    "    image_gray_array = np.array(image_gray)\n",
    "    random_color = random.randint(0, 255)  # 랜덤한 그레이스케일 값 생성\n",
    "    # 마스크 영역에 랜덤 색상 적용\n",
    "    image_gray_array[full_image_mask == 1] = random_color\n",
    "    image_gray_masked = Image.fromarray(image_gray_array)\n",
    "\n",
    "    # 결과 반환\n",
    "    return {\n",
    "        'image_gt': image,               # 원본 이미지\n",
    "        'mask': full_image_mask,         # 생성된 마스크\n",
    "        'image_gray': image_gray,        # 그레이스케일 이미지\n",
    "        'image_gray_masked': image_gray_masked  # 마스크가 적용된 그레이스케일 이미지\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ssim_score(true, pred):\n",
    "    # 전체 RGB 이미지를 사용해 SSIM 계산 (channel_axis=-1)\n",
    "    ssim_value = ski_ssim(true, pred, channel_axis=-1, data_range=pred.max() - pred.min())\n",
    "    return ssim_value\n",
    "\n",
    "def get_masked_ssim_score(true, pred, mask):\n",
    "    # 손실 영역의 좌표에서만 RGB 채널별 픽셀 값 추출\n",
    "    true_masked_pixels = true[mask > 0]\n",
    "    pred_masked_pixels = pred[mask > 0]\n",
    "\n",
    "    # 손실 영역 픽셀만으로 SSIM 계산 (채널축 사용)\n",
    "    ssim_value = ski_ssim(\n",
    "        true_masked_pixels,\n",
    "        pred_masked_pixels,\n",
    "        channel_axis=-1,\n",
    "        data_range=pred.max() - pred.min()\n",
    "    )\n",
    "    return ssim_value\n",
    "\n",
    "def get_histogram_similarity(true, pred, cvt_color=cv2.COLOR_RGB2HSV):\n",
    "    # BGR 이미지를 HSV로 변환\n",
    "    true_hsv = cv2.cvtColor(true, cvt_color)\n",
    "    pred_hsv = cv2.cvtColor(pred, cvt_color)\n",
    "\n",
    "    # H 채널에서 히스토그램 계산 및 정규화\n",
    "    hist_true = cv2.calcHist([true_hsv], [0], None, [180], [0, 180])\n",
    "    hist_pred = cv2.calcHist([pred_hsv], [0], None, [180], [0, 180])\n",
    "    hist_true = cv2.normalize(hist_true, hist_true).flatten()\n",
    "    hist_pred = cv2.normalize(hist_pred, hist_pred).flatten()\n",
    "\n",
    "    # 히스토그램 간 유사도 계산 (상관 계수 사용)\n",
    "    similarity = cv2.compareHist(hist_true, hist_pred, cv2.HISTCMP_CORREL)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험의 재현성을 위한 랜덤 시드 설정\n",
    "SEED = 42\n",
    "# K-fold 교차 검증을 위한 분할 수\n",
    "N_SPLIT = 5\n",
    "# 모델 학습시 한 번에 처리할 데이터 개수\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 8  # Ryzen 5800x의 코어 수를 고려\n",
    "PIN_MEMORY = True  # GPU 메모리 전송 최적화\n",
    "# 이미지 전처리를 위한 정규화 파라미터\n",
    "# 일반적으로 ImageNet 데이터셋의 평균과 표준편차 값을 사용\n",
    "IMAGE_PREPROC_MEAN = 0.5    # 이미지 픽셀값의 평균\n",
    "IMAGE_PREPROC_STD = 0.225   # 이미지 픽셀값의 표준편차\n",
    "\n",
    "# 다각형 마스크 생성시 최소 바운딩 박스 크기\n",
    "MIN_POLYGON_BBOX_SIZE = 64  # 픽셀 단위\n",
    "\n",
    "# 학습 관련 파라미터 (새로 추가)\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정\n",
    "TRAIN_DATA_DIR = '/home/kwy00/song/lama-with-refiner/extracted_files/train_gt'  # 학습용 원본 이미지 경로\n",
    "VALID_DATA_DIR = f'/home/kwy00/song/data/valid_input/{SEED=}-{MIN_POLYGON_BBOX_SIZE=}'  # 검증 데이터 경로\n",
    "TEST_DATA_DIR = '/home/kwy00/song/lama-with-refiner/extracted_files/test_input'  # 테스트 데이터 경로\n",
    "SUBMISSON_DATA_DIR = '/home/kwy00/song/lama-with-refiner/submission'  # 제출 파일 저장 경로\n",
    "\n",
    "# 실험 설정\n",
    "EXPERIMENT_NAME = 'seventh'  # 현재 실험의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/home/kwy00/song/train_preproc.csv')\n",
    "test_df = pd.read_csv('/home/kwy00/song/test_preproc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터 저장을 위한 디렉토리 생성\n",
    "os.makedirs(VALID_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# 학습 데이터프레임의 모든 이미지에 대해 반복\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    # 원본 이미지 경로 가져오기\n",
    "    img_path = train_df.iloc[idx, 0]\n",
    "    img_path = os.path.join(TRAIN_DATA_DIR, img_path)\n",
    "\n",
    "    # 저장할 파일명 생성 (TRAIN -> VALID, png -> npy로 변환)\n",
    "    save_image_name = os.path.basename(img_path).replace('TRAIN', 'VALID').replace('png','npy')\n",
    "    save_image_path = f'{VALID_DATA_DIR}/{save_image_name}'\n",
    "\n",
    "    # 이미 처리된 파일은 건너뛰기\n",
    "    if os.path.exists(save_image_path):\n",
    "        continue\n",
    "\n",
    "    # 이미지 열기 및 마스크 생성\n",
    "    image = Image.open(img_path)\n",
    "    valid_input_image = get_input_image(image, MIN_POLYGON_BBOX_SIZE)\n",
    "\n",
    "    # 생성된 데이터를 numpy 배열로 저장\n",
    "    np.save(save_image_path, valid_input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_outlier = train_df[train_df['label']==-1]\n",
    "train_df = train_df[train_df['label']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=N_SPLIT, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold 교차 검증을 위한 데이터 분할\n",
    "for fold_idx, (train_indices, valid_indices) in enumerate(kf.split(train_df['image'], train_df['label'])):\n",
    "    # 학습용 데이터와 검증용 데이터 분리\n",
    "    train_fold_df = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "    valid_fold_df = train_df.iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "    # 검증 데이터의 파일명 변환 (TRAIN -> VALID, png -> npy)\n",
    "    valid_fold_df['image'] = valid_fold_df['image'].apply(lambda x: x.replace('TRAIN', 'VALID').replace('png', 'npy'))\n",
    "\n",
    "    # 검증 속도 향상을 위해 각 레이블당 하나의 샘플만 유지\n",
    "    valid_fold_df = valid_fold_df.drop_duplicates('label')\n",
    "    # train_fold_df = pd.concat([train_fold_df,train_df_outlier],axis=0).reset_index(drop=True)\n",
    "    # 첫 번째 폴드만 사용\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LaMa git clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/geomagical/lama-with-refiner.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. train 데이터셋으로 학습시킨 모델로 만든 test 데이터셋 마스크 이미지 다운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_file(zip_file_path, extract_to_path):\n",
    "    \"\"\"\n",
    "    ZIP 파일을 지정된 폴더에 압축 해제하는 함수.\n",
    "\n",
    "    Args:\n",
    "        zip_file_path (str): 압축 해제할 ZIP 파일 경로.\n",
    "        extract_to_path (str): 파일을 풀어놓을 폴더 경로.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # ZIP 파일 열기\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # 압축 해제\n",
    "        zip_ref.extractall(extract_to_path)\n",
    "    print(f\"압축 해제 완료: {extract_to_path}\")\n",
    "\n",
    "# 사용 예시\n",
    "zip_file_path = \"/home/kwy00/song/predicted_mask3.zip\"\n",
    "extract_to_path =\"./lama-with-refiner/predicted_masks3\"\n",
    "unzip_file(zip_file_path, extract_to_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 데이터 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, data_dir='/home/kwy00/song/content/extracted_files/train_gt', mode='train',mask_dir = \"/home/kwy00/song/lama-with-refiner/predicted_masks3\" ,min_polygon_bbox_size=MIN_POLYGON_BBOX_SIZE):\n",
    "        self.df = df\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.min_polygon_bbox_size = min_polygon_bbox_size\n",
    "        self.mask_dir = mask_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path and label\n",
    "        img_path = self.df.iloc[idx, 0]  # Assuming first column is the path\n",
    "        img_path = os.path.join(self.data_dir, img_path)\n",
    "\n",
    "        # Apply augmentation if in training mode\n",
    "        if self.mode == 'train':\n",
    "            image = Image.open(img_path)\n",
    "            image_input = get_input_image(image, self.min_polygon_bbox_size)\n",
    "            return image_input\n",
    "\n",
    "        elif self.mode == 'valid':\n",
    "            image_input = self.load_input_image(img_path)\n",
    "            return image_input\n",
    "        elif self.mode == 'test':\n",
    "            image = Image.open(img_path)\n",
    "            img_name = os.path.basename(img_path)\n",
    "            mask_path = os.path.join(self.mask_dir, img_name)\n",
    "            mask = Image.open(mask_path)\n",
    "            return {\n",
    "                'image_gray_masked':image,\n",
    "                'mask':mask\n",
    "            }\n",
    "\n",
    "    def load_input_image(self, img_input_path):\n",
    "        image_input = np.load(img_input_path, allow_pickle=True)\n",
    "        return image_input.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(train_fold_df, data_dir=TRAIN_DATA_DIR, mode='train')\n",
    "valid_dataset = CustomImageDataset(valid_fold_df, data_dir=VALID_DATA_DIR, mode='valid')\n",
    "test_dataset = CustomImageDataset(test_df, data_dir=TEST_DATA_DIR, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFn:\n",
    "    def __init__(self, mean=IMAGE_PREPROC_MEAN, std=IMAGE_PREPROC_STD, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        if self.mode =='train' or self.mode=='valid':\n",
    "            # Initialize lists to store each component of the batch\n",
    "            masks= []\n",
    "            images_gray = []\n",
    "            images_gray_masked = []\n",
    "            images_gt = []\n",
    "\n",
    "            for example in examples:\n",
    "                # Assuming each example is a dictionary with keys 'mask', 'image_gray', 'image_gray_masked', 'image_gt'\n",
    "                masks.append(example['mask'])\n",
    "                images_gray.append(self.normalize_image(example['image_gray']))\n",
    "                images_gray_masked.append(self.normalize_image(example['image_gray_masked']))\n",
    "                images_gt.append(self.normalize_image(np.array(example['image_gt'])))\n",
    "\n",
    "            return {\n",
    "                'masks': torch.from_numpy(np.stack(masks)).long(),\n",
    "                'images_gray': torch.from_numpy(np.stack(images_gray)).unsqueeze(1).float(),\n",
    "                'images_gray_masked': torch.from_numpy(np.stack(images_gray_masked)).unsqueeze(1).float(),\n",
    "                'images_gt': torch.from_numpy(np.stack(images_gt)).permute(0,3,1,2).float()\n",
    "            }\n",
    "\n",
    "        elif self.mode == 'test':\n",
    "            images_gray_masked = []\n",
    "            masks=[]\n",
    "            for example in examples:\n",
    "                images_gray_masked.append(self.normalize_image(example['image_gray_masked']))\n",
    "                masks.append(example['mask'])\n",
    "            return {\n",
    "                'images_gray_masked': torch.from_numpy(np.stack(images_gray_masked)).unsqueeze(1).float(),\n",
    "                'mask': torch.from_numpy(np.stack(masks)).long()\n",
    "            }\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        return (np.array(image)/255-self.mean)/self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kwy00/song/lama-with-refiner\n"
     ]
    }
   ],
   "source": [
    "%cd lama-with-refiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,  # Ryzen 5800x의 8코어 활용\n",
    "    pin_memory=True,  # GPU 메모리 전송 최적화\n",
    "    collate_fn=CollateFn(mode='train'),\n",
    "    persistent_workers=True  # 워커 재사용으로 성능 향상\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE*2,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=CollateFn(mode='valid'),\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE*2,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=CollateFn(mode='test'),\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. LaMa pretrained 된 가중치 다운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -LJO https://huggingface.co/smartywu/big-lama/resolve/main/big-lama.zip\n",
    "!unzip big-lama.zip -d ./weights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!mkdir -p ade20k/ade20k-resnet50dilated-ppm_deepsup/\n",
    "!wget -P ade20k/ade20k-resnet50dilated-ppm_deepsup/ http://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. LaMa에 필요한 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saicinpainting.training.trainers import make_training_model\n",
    "from saicinpainting.training.trainers import load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "from saicinpainting.training.trainers import make_training_model\n",
    "from saicinpainting.utils import register_debug_signal_handlers, handle_ddp_subprocess, handle_ddp_parent_process, \\\n",
    "    handle_deterministic_config\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path='./weights/big-lama'\n",
    "config_name='config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(os.path.join(config_path, config_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from saicinpainting.training.trainers.default import DefaultInpaintingTrainingModule\n",
    "\n",
    "IMAGE_PREPROC_MEAN=0.5\n",
    "IMAGE_PREPROC_STD=0.225"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 평가지표 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ski_ssim\n",
    "import cv2\n",
    "def get_ssim_score(true, pred):\n",
    "    # 전체 RGB 이미지를 사용해 SSIM 계산 (channel_axis=-1)\n",
    "    ssim_value = ski_ssim(true, pred, channel_axis=-1, data_range=pred.max() - pred.min())\n",
    "    return ssim_value\n",
    "\n",
    "def get_masked_ssim_score(true, pred, mask):\n",
    "    # 손실 영역의 좌표에서만 RGB 채널별 픽셀 값 추출\n",
    "    true_masked_pixels = true[mask > 0]\n",
    "    pred_masked_pixels = pred[mask > 0]\n",
    "\n",
    "    # 손실 영역 픽셀만으로 SSIM 계산 (채널축 사용)\n",
    "    ssim_value = ski_ssim(\n",
    "        true_masked_pixels,\n",
    "        pred_masked_pixels,\n",
    "        channel_axis=-1,\n",
    "        data_range=pred.max() - pred.min()\n",
    "    )\n",
    "    return ssim_value\n",
    "\n",
    "def get_histogram_similarity(true, pred, cvt_color=cv2.COLOR_RGB2HSV):\n",
    "    # BGR 이미지를 HSV로 변환\n",
    "    true_hsv = cv2.cvtColor(true, cvt_color)\n",
    "    pred_hsv = cv2.cvtColor(pred, cvt_color)\n",
    "\n",
    "    # H 채널에서 히스토그램 계산 및 정규화\n",
    "    hist_true = cv2.calcHist([true_hsv], [0], None, [180], [0, 180])\n",
    "    hist_pred = cv2.calcHist([pred_hsv], [0], None, [180], [0, 180])\n",
    "    hist_true = cv2.normalize(hist_true, hist_true).flatten()\n",
    "    hist_pred = cv2.normalize(hist_pred, hist_pred).flatten()\n",
    "\n",
    "    # 히스토그램 간 유사도 계산 (상관 계수 사용)\n",
    "    similarity = cv2.compareHist(hist_true, hist_pred, cv2.HISTCMP_CORREL)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. LaMa용 로드 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from saicinpainting.training.trainers.default import DefaultInpaintingTrainingModule\n",
    "\n",
    "\n",
    "def get_training_model_class(kind):\n",
    "    if kind == 'default':\n",
    "        return DefaultInpaintingTrainingModule\n",
    "\n",
    "    raise ValueError(f'Unknown trainer module {kind}')\n",
    "\n",
    "\n",
    "def make_training_model(config):\n",
    "    kind = config.training_model.kind\n",
    "    kwargs = dict(config.training_model)\n",
    "    kwargs.pop('kind')\n",
    "    kwargs['use_ddp'] = config.trainer.kwargs.get('accelerator', None) == 'ddp'\n",
    "\n",
    "    logging.info(f'Make training model {kind}')\n",
    "\n",
    "    cls = get_training_model_class(kind)\n",
    "    return cls(config, **kwargs)\n",
    "\n",
    "\n",
    "def load_checkpoint(train_config, path, map_location='cuda', strict=True):\n",
    "    model: torch.nn.Module = make_training_model(train_config)\n",
    "    state = torch.load(path, map_location=map_location)\n",
    "    model.load_state_dict(state['state_dict'], strict=strict)\n",
    "    model.on_load_checkpoint(state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 텐서보드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 45399 (pid 57988), started 17:04:00 ago. (Use '!kill 57988' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 45399;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/ --port=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. 체크포인트와 earlystoppig 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# 체크포인트 저장 경로와 규칙 정의\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./checkpointss/\",           # 체크포인트 저장 디렉토리\n",
    "    filename=f'best2-{fold_idx=}-{SEED=}'+'-{epoch:02d}-{val_score:.4f}',  # 파일 이름 형식\n",
    "    save_top_k=1,                     # 가장 낮은 검증 손실을 기록한 3개만 저장\n",
    "    monitor=\"val_score\",               # 모니터링할 메트릭\n",
    "    mode=\"max\",                       # 손실이 작을수록 좋음\n",
    "    save_weights_only=False,          # 전체 모델 상태를 저장\n",
    "    verbose=True                      # 저장 시 메시지 출력\n",
    ")\n",
    "earlystopping_callback = EarlyStopping(monitor=\"val_score\",min_delta=1e-4, mode=\"max\", patience=5,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "trainer = Trainer(max_epochs=40, precision='32', callbacks=[checkpoint_callback,earlystopping_callback,], detect_anomaly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # 메모리 단편화 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saicinpainting.training.trainers import load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. 합친 모델 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "model_2 = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b3\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,\n",
    "    classes=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning\n",
    "class placemodel(lightning.pytorch.LightningModule):\n",
    "  def __init__(self,  image_mean=IMAGE_PREPROC_MEAN, image_std=IMAGE_PREPROC_STD):\n",
    "    super().__init__()\n",
    "    self.training = True\n",
    "    checkpoint_path = \"/home/kwy00/song/lama-with-refiner/weights/big-lama/models/best.ckpt\"\n",
    "    self.model_1 = load_checkpoint(config,checkpoint_path,strict=False)\n",
    "    # 모든 파라미터 고정\n",
    "    for param in self.model_1.generator.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in self.model_1.generator.model[22].parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in self.model_1.discriminator.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 특정 레이어만 학습 가능\n",
    "    \n",
    "    self.model_2 = model_2\n",
    "    self.image_mean=image_mean\n",
    "    self.image_std=image_std\n",
    "\n",
    "  def forward(self, images_gray_masked):\n",
    "      if self.training == True:\n",
    "\n",
    "\n",
    "        images_gray_masked['image'] = torch.cat([images_gray_masked['image'],images_gray_masked['image'],images_gray_masked['image']],dim=1)\n",
    "        igm = self.model_1(images_gray_masked)\n",
    "\n",
    "        rgb_images = igm['inpainted']\n",
    "        rgb_onechannel = rgb_images[:, 0:1, :, :]\n",
    "\n",
    "        mask_min =rgb_onechannel.min()\n",
    "        mask_max =rgb_onechannel.max()\n",
    "\n",
    "        # 정규화 해제하여 원래 값 복원\n",
    "        images_gray_restored= rgb_onechannel * (mask_max - mask_min + 1e-8) + mask_min\n",
    "\n",
    "\n",
    "\n",
    "        images_restored = self.model_2(rgb_onechannel)\n",
    "        #텐서보드에 이미지 볼수있는 코드 \n",
    "        #self.logger.experiment.add_images('Training/Model2 output', images_restored, self.global_step) \n",
    "        return rgb_onechannel, images_restored\n",
    "      else :\n",
    "        with torch.no_grad():\n",
    "\n",
    "            images_gray_masked['image'] = torch.cat([images_gray_masked['image'],images_gray_masked['image'],images_gray_masked['image']],dim=1)\n",
    "            \n",
    "            #self.logger.experiment.add_images('Testing/Model1 mask', images_gray_masked['mask'], self.global_step)\n",
    "            igm = self.model_1(images_gray_masked)\n",
    "            rgb_images = igm['inpainted']\n",
    "            #self.logger.experiment.add_images('Testing/Model1 output', rgb_images[:, 0:1, :, :], self.global_step)\n",
    "            rgb_onechannel = rgb_images[:, 0:1, :, :]\n",
    "            images_gray_restored = rgb_onechannel\n",
    "\n",
    "            mask_min =images_gray_restored.min()\n",
    "            mask_max =images_gray_restored.max()\n",
    "\n",
    "            # 정규화 해제하여 원래 값 복원\n",
    "            images_gray_restored= images_gray_restored * (mask_max - mask_min + 1e-8) + mask_min\n",
    "            images_restored = self.model_2(images_gray_restored)\n",
    "            #self.logger.experiment.add_images('Testing/Model2 output', images_restored, self.global_step)\n",
    "        return rgb_onechannel, images_restored\n",
    "\n",
    "\n",
    "  def unnormalize(self, output, round=False):\n",
    "        image_restored = ((output*self.image_std+self.image_mean)*255).clamp(0,255)\n",
    "        if round:\n",
    "            image_restored = torch.round(image_restored)\n",
    "        return image_restored\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_score\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "        self.training = True\n",
    "        opt = self.optimizers()\n",
    "        opt.zero_grad()\n",
    "        batch['masks'] = (batch['masks'] -batch['masks'].min()) / (batch['masks'].max() - batch['masks'].min() + 1e-8)\n",
    "        batch['images_gray_masked'] = (batch['images_gray_masked'] -batch['images_gray_masked'].min()) / (batch['images_gray_masked'].max() - batch['images_gray_masked'].min() + 1e-8)\n",
    "        batch['images_gray'] = (batch['images_gray'] -batch['images_gray'].min()) / (batch['images_gray'].max() -batch['images_gray'].min() + 1e-8)\n",
    "        batch['images_gt'] = (batch['images_gt'] -batch['images_gt'].min()) / (batch['images_gt'].max() -batch['images_gt'].min() + 1e-8)\n",
    "\n",
    "        masks, images_gray_masked, images_gray, images_gt = batch['masks'], batch['images_gray_masked'], batch['images_gray'], batch['images_gt']\n",
    "        batch2 ={}\n",
    "\n",
    "        batch2['image'] = batch['images_gray']\n",
    "        batch2['mask'] = batch['masks']\n",
    "        batch2['mask'] = batch2['mask'].unsqueeze(1)\n",
    "\n",
    "\n",
    "        images_gray_restored, images_restored = self(batch2)\n",
    "\n",
    "        mask_min =images_gray_restored.min()\n",
    "        mask_max =images_gray_restored.max()\n",
    "\n",
    "        # 정규화 해제하여 원래 값 복원\n",
    "        images_gray_restored= images_gray_restored * (mask_max - mask_min + 1e-8) + mask_min\n",
    "\n",
    "\n",
    "        mask_min =images_gray.min()\n",
    "        mask_max =images_gray.max()\n",
    "\n",
    "        # 정규화 해제하여 원래 값 복원\n",
    "        images_gray = images_gray * (mask_max - mask_min + 1e-8) + mask_min\n",
    "\n",
    "        loss_pixel_gray = F.l1_loss(images_gray, images_gray_restored, reduction='mean') * 0.3 + F.mse_loss(images_gray, images_gray_restored, reduction='mean') * 0.7\n",
    "        loss_pixel = F.l1_loss(images_gt, images_restored, reduction='mean') * 0.3 + F.mse_loss(images_gt, images_restored, reduction='mean') * 0.7\n",
    "        loss = loss_pixel_gray * 0.3 + loss_pixel * 0.7\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=False)\n",
    "        self.log(\"train_loss_pixel_gray\", loss_pixel_gray, on_step=True, on_epoch=False)\n",
    "        self.log(\"train_loss_pixel\", loss_pixel, on_step=True, on_epoch=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "      self.training = True\n",
    "      batch['images_gray_masked'] = (batch['images_gray_masked'] -batch['images_gray_masked'].min()) / (batch['images_gray_masked'].max() - batch['images_gray_masked'].min() + 1e-8)\n",
    "      batch['images_gt'] = (batch['images_gt'] -batch['images_gt'].min()) / (batch['images_gt'].max() -batch['images_gt'].min() + 1e-8)\n",
    "\n",
    "      masks, images_gray_masked, images_gt = batch['masks'], batch['images_gray_masked'], batch['images_gt']\n",
    "\n",
    "      batch2 = {}\n",
    "      batch2['image'] = batch['images_gray_masked']\n",
    "      batch2['mask'] = batch['masks']\n",
    "      batch2['mask'] = batch2['mask'].unsqueeze(1)  # (Batch, 1, Height, Width)\n",
    "\n",
    "      images_gray_restored, images_restored = self(batch2)\n",
    "\n",
    "      images_gt, images_restored = self.unnormalize(images_gt, round=True), self.unnormalize(images_restored, round=True)\n",
    "      masks_np = masks.detach().cpu().numpy()\n",
    "      images_gt_np = images_gt.detach().cpu().permute(0,2,3,1).float().numpy().astype(np.uint8)\n",
    "      images_restored_np = images_restored.detach().cpu().permute(0,2,3,1).float().numpy().astype(np.uint8)\n",
    "      total_ssim_score = 0\n",
    "      masked_ssim_score = 0\n",
    "      hist_sim_score = 0\n",
    "      for image_gt_np, image_restored_np, mask_np in zip(images_gt_np, images_restored_np, masks_np):\n",
    "          total_ssim_score += get_ssim_score(image_gt_np, image_restored_np) / len(images_gt)\n",
    "          masked_ssim_score += get_masked_ssim_score(image_gt_np, image_restored_np, mask_np)/ len(images_gt)\n",
    "          hist_sim_score += get_histogram_similarity(image_gt_np, image_restored_np, cv2.COLOR_RGB2HSV)/ len(images_gt)\n",
    "      score = total_ssim_score * 0.2 + masked_ssim_score * 0.4 + hist_sim_score * 0.4\n",
    "      self.log(f\"val_score\", score, on_step=False, on_epoch=True)\n",
    "      self.log(f\"val_total_ssim_score\", total_ssim_score, on_step=False, on_epoch=True)\n",
    "      self.log(f\"val_masked_ssim_score\", masked_ssim_score, on_step=False, on_epoch=True)\n",
    "      self.log(f\"val_hist_sim_score\", hist_sim_score, on_step=False, on_epoch=True)\n",
    "\n",
    "\n",
    "      return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def predict_step(self, batch, batch_idx):\n",
    "        self.training = False\n",
    "        batch['images_gray_masked'] = (batch['images_gray_masked'] -batch['images_gray_masked'].min()) / (batch['images_gray_masked'].max() - batch['images_gray_masked'].min() + 1e-8)\n",
    "        batch['mask'] = (batch['mask'] -batch['mask'].min()) / (batch['mask'].max() - batch['mask'].min() + 1e-8)\n",
    "        images_gray_masked = batch['images_gray_masked']\n",
    "        batch2 ={}\n",
    "        batch2['image'] = images_gray_masked\n",
    "\n",
    "  \n",
    "        batch2['mask'] = batch['mask'].unsqueeze(1)\n",
    "\n",
    "        images_gray_restored, images_restored = self(batch2)\n",
    "        #self.logger.experiment.add_images('Training/Model1 images_restored', images_restored, self.global_step)\n",
    "        images_restored = self.unnormalize(images_restored, round=True)\n",
    "\n",
    "        #self.logger.experiment.add_images('Training/Model1 unnormalize', images_restored, self.global_step)\n",
    "        images_restored_np = images_restored.detach().cpu().permute(0,2,3,1).float().numpy().astype(np.uint8)\n",
    "        #self.logger.experiment.add_images('Training/Model1 images_restored', images_restored_np, self.global_step)\n",
    "        return images_restored_np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PermissionError: Unable to create directory at /group-volume/User-Driven-Content-Generation/r.suvorov/inpainting/experiments/r.suvorov_2021-04-30_14-41-12_train_simple_pix2pix2_gap_sdpl_novgg_large_b18_ffc075_batch8x15/samples. Using fallback path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/kwy00/song/lama-with-refiner/saicinpainting/evaluation/losses/lpips.py:294: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(model_path, **kw), strict=False)\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for net_encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwy00/song/lama-with-refiner/models/ade20k/base.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n",
      "/home/kwy00/song/lama-with-refiner/saicinpainting/training/trainers/__init__.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=map_location)\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /home/kwy00/song/lama-with-refiner/checkpointss exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name    | Type                            | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | model_1 | DefaultInpaintingTrainingModule | 132 M  | train\n",
      "1 | model_2 | Unet                            | 13.2 M | train\n",
      "--------------------------------------------------------------------\n",
      "15.8 M    Trainable params\n",
      "130 M     Non-trainable params\n",
      "145 M     Total params\n",
      "583.786   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2738/2738 [54:33<00:00,  0.84it/s, v_num=22]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_score improved. New best score: 0.513\n",
      "Epoch 0, global step 2738: 'val_score' reached 0.51333 (best 0.51333), saving model to '/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=00-val_score=0.5133.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2738/2738 [54:04<00:00,  0.84it/s, v_num=22]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_score improved by 0.031 >= min_delta = 0.0001. New best score: 0.544\n",
      "Epoch 1, global step 5476: 'val_score' reached 0.54442 (best 0.54442), saving model to '/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=01-val_score=0.5444.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2738/2738 [54:01<00:00,  0.84it/s, v_num=22]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_score improved by 0.019 >= min_delta = 0.0001. New best score: 0.563\n",
      "Epoch 2, global step 8214: 'val_score' reached 0.56333 (best 0.56333), saving model to '/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=02-val_score=0.5633.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2738/2738 [54:00<00:00,  0.84it/s, v_num=22]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_score improved by 0.013 >= min_delta = 0.0001. New best score: 0.577\n",
      "Epoch 3, global step 10952: 'val_score' reached 0.57651 (best 0.57651), saving model to '/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=03-val_score=0.5765.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2738/2738 [54:02<00:00,  0.84it/s, v_num=22]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_score improved by 0.009 >= min_delta = 0.0001. New best score: 0.586\n",
      "Epoch 4, global step 13690: 'val_score' reached 0.58566 (best 0.58566), saving model to '/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=04-val_score=0.5857.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2738/2738 [54:01<00:00,  0.84it/s, v_num=22]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_score improved by 0.007 >= min_delta = 0.0001. New best score: 0.593\n",
      "Epoch 5, global step 16428: 'val_score' reached 0.59303 (best 0.59303), saving model to '/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=05-val_score=0.5930.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2738/2738 [54:03<00:00,  0.84it/s, v_num=22]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_score improved by 0.008 >= min_delta = 0.0001. New best score: 0.601\n",
      "Epoch 6, global step 19166: 'val_score' reached 0.60092 (best 0.60092), saving model to '/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=06-val_score=0.6009.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  20%|█▉        | 545/2738 [15:24<1:01:59,  0.59it/s, v_num=22]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.fit(placemodel(), train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################재학습코드#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PermissionError: Unable to create directory at /group-volume/User-Driven-Content-Generation/r.suvorov/inpainting/experiments/r.suvorov_2021-04-30_14-41-12_train_simple_pix2pix2_gap_sdpl_novgg_large_b18_ffc075_batch8x15/samples. Using fallback path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/kwy00/song/lama-with-refiner/saicinpainting/evaluation/losses/lpips.py:294: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(model_path, **kw), strict=False)\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for net_encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwy00/song/lama-with-refiner/models/ade20k/base.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n",
      "/home/kwy00/song/lama-with-refiner/saicinpainting/training/trainers/__init__.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=map_location)\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /home/kwy00/song/lama-with-refiner/checkpointss exists and is not empty.\n",
      "Restoring states from the checkpoint path at /home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=07-val_score=0.6105.ckpt\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/lightning/fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name    | Type                            | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | model_1 | DefaultInpaintingTrainingModule | 132 M  | train\n",
      "1 | model_2 | Unet                            | 13.2 M | train\n",
      "--------------------------------------------------------------------\n",
      "15.8 M    Trainable params\n",
      "130 M     Non-trainable params\n",
      "145 M     Total params\n",
      "583.786   Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at /home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=07-val_score=0.6105.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2738/2738 [49:37<00:00,  0.92it/s, v_num=24]     "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "# Path to the checkpoint file\n",
    "ckpt_path = '/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=07-val_score=0.6105.ckpt' #'/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=06-val_score=0.6009.ckpt'#'/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=07-val_score=0.6086.ckpt'#\"/content/lama-with-refiner/check/epoch=05-val_score=0.5443.ckpt\" #\"/content/drive/MyDrive/딥러닝심화/이미지복원/epoch=04-val_score=0.5106.ckpt\"\n",
    "\n",
    "# Initialize the trainer and resume from checkpoint\n",
    "trainer = Trainer(\n",
    "    max_epochs=40,\n",
    "    precision='32',\n",
    "    callbacks=[checkpoint_callback,earlystopping_callback,],\n",
    "    detect_anomaly=False\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "trainer.fit(placemodel(), train_dataloader, valid_dataloader, ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. 학습된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/lightning/fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PermissionError: Unable to create directory at /group-volume/User-Driven-Content-Generation/r.suvorov/inpainting/experiments/r.suvorov_2021-04-30_14-41-12_train_simple_pix2pix2_gap_sdpl_novgg_large_b18_ffc075_batch8x15/samples. Using fallback path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/kwy00/song/lama-with-refiner/saicinpainting/evaluation/losses/lpips.py:294: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(model_path, **kw), strict=False)\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/kwy00/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for net_encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwy00/song/lama-with-refiner/models/ade20k/base.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n",
      "/home/kwy00/song/lama-with-refiner/saicinpainting/training/trainers/__init__.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "lit_ir_model = placemodel.load_from_checkpoint(\n",
    "    '/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=07-val_score=0.6105.ckpt',\n",
    "    #'/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=12-val_score=0.6332.ckpt',\n",
    "    #'/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=07-val_score=0.6086.ckpt',\n",
    "    #'/home/kwy00/song/lama-with-refiner/checkpointss/best2-fold_idx=0-SEED=42-epoch=01-val_score=0.5435.ckpt',\n",
    "    model_2=model_2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. 추론 및 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:09<00:00,  0.78it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(lit_ir_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_dir = os.path.join(SUBMISSON_DATA_DIR, EXPERIMENT_NAME)\n",
    "submission_file = f'{SUBMISSON_DATA_DIR}/{EXPERIMENT_NAME}.zip'\n",
    "os.makedirs(submission_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    image_pred = Image.fromarray(predictions[idx])\n",
    "    image_pred.save(os.path.join(submission_dir, row['image']), \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compress the directory into a ZIP file using glob\n",
    "with zipfile.ZipFile(submission_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file_path in glob(f\"{submission_dir}/*.png\"):\n",
    "        arcname = os.path.relpath(file_path, submission_dir)\n",
    "        zipf.write(file_path, arcname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "song",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
